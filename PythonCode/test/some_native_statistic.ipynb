{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (5,6,7,9,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from snownlp import SnowNLP\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train=pd.read_csv('input/train.csv')\n",
    "test=pd.read_csv('input/test.csv')\n",
    "\n",
    "#print train['question1']\n",
    "\n",
    "train_qs=pd.Series(train['question1'].tolist()+\n",
    "                   train['question2'].tolist()).astype(str)\n",
    "\n",
    "test_qs=pd.Series(test['question1'].tolist()+\n",
    "                  test['question2'].tolist()).astype(str)\n",
    "\n",
    "stops=set(stopwords.words(\"english\"))\n",
    "\n",
    "#计算两句话的共有词\n",
    "def word_match_share(row):\n",
    "    q1words={}\n",
    "    q2words={}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:#如果不是stopwords则存入q1words=>(key,value)\n",
    "            q1words[word]=1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word]=1\n",
    "    if len(q1words)==0 or len(q2words)==0:\n",
    "        return 0\n",
    "    shared_words_q1=[w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_q2=[w for w in q2words.keys() if w in q1words]\n",
    "    R=(len(shared_words_q1)+len(shared_words_q2)+0.0)/(len(q1words)+len(q2words))\n",
    "    return R\n",
    "\n",
    "def get_weight(count,eps=10000,min_count=2):\n",
    "    if count<min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1.0/(count+eps)\n",
    "\n",
    "eps=5000\n",
    "words=(\" \".join(train_qs)).lower().split()\n",
    "counts=Counter(words)\n",
    "weights={word:get_weight(count) for word,count in counts.items()}\n",
    "    \n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "        \n",
    "    R = (np.sum(shared_weights)+0.0) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def sentence_sentiment_diff(row):\n",
    "    s1=SnowNLP(str(row['question1'])).sentiments\n",
    "    s2=SnowNLP(str(row['question2'])).sentiments\n",
    "    return (s1-s2)*(s1-s2)\n",
    "\n",
    "\n",
    "print 'over'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 'construct dataset'\n",
    "x_train = pd.DataFrame()\n",
    "x_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct train data feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:74: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "#构造train,valid,test set\n",
    "\n",
    "print 'construct train data feature'\n",
    "\n",
    "x_train['word_match'] = train.apply(word_match_share,axis=1,raw=True)\n",
    "x_train['tfidf_word_match'] = train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "\n",
    "print 'over'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct test data feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:74: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "print 'construct test data feature'\n",
    "\n",
    "x_test['word_match'] = test.apply(word_match_share, axis=1, raw=True)\n",
    "x_test['tfidf_word_match'] = test.apply(tfidf_word_match_share,axis=1,raw=True)\n",
    "\n",
    "print 'over'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_sentiment\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "print 'sentence_sentiment'\n",
    "\n",
    "x_train['sentiment']=train.apply(sentence_sentiment_diff,axis=1,raw=True)\n",
    "x_test['sentiment']=train.apply(sentence_sentiment_diff,axis=1,raw=True)\n",
    "\n",
    "print 'over'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic feature \n",
      "training data is over\n",
      "test data is over\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import math\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "\n",
    "print 'basic feature '\n",
    "\n",
    "#x_train['len_q1'] = train.question1.apply(lambda x: len(str(x)))\n",
    "#x_train['len_q2'] = train.question2.apply(lambda x: len(str(x)))\n",
    "#x_train['len_char_q1'] = train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "#x_train['len_char_q2'] = train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "#x_train['len_word_q1'] = train.question1.apply(lambda x: len(str(x).split()))\n",
    "#x_train['len_word_q2'] = train.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "len_q1=train.question1.apply(lambda x: len(str(x)))\n",
    "len_q2=train.question2.apply(lambda x: len(str(x)))\n",
    "x_train['diff_len'] = abs(len_q1-len_q2)\n",
    "\n",
    "len_char_q1=train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "len_char_q2=train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "x_train['diff_len_char']=abs(len_char_q1-len_char_q2)\n",
    "\n",
    "len_word_q1=train.question1.apply(lambda x: len(str(x).split()))\n",
    "len_word_q2=train.question2.apply(lambda x: len(str(x).split()))\n",
    "x_train['diff_len_word']=abs(len_word_q1-len_word_q2)\n",
    "\n",
    "#x_train['common_words'] = train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "\n",
    "x_train['fuzz_qratio'] = train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_train['fuzz_WRatio'] = train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_train['fuzz_partial_ratio'] = train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_train['fuzz_partial_token_set_ratio'] = train.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_train['fuzz_partial_token_sort_ratio'] = train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_train['fuzz_token_set_ratio'] = train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_train['fuzz_token_sort_ratio'] = train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "\n",
    "print 'training data is over'\n",
    "\n",
    "#x_test['len_q1'] = test.question1.apply(lambda x: len(str(x)))\n",
    "#x_test['len_q2'] = test.question2.apply(lambda x: len(str(x)))\n",
    "#x_test['diff_len'] = x_test.len_q1 - x_test.len_q2\n",
    "#x_test['len_char_q1'] = test.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "#x_test['len_char_q2'] = test.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "#x_test['len_word_q1'] = test.question1.apply(lambda x: len(str(x).split()))\n",
    "#x_test['len_word_q2'] = test.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "len_q1=test.question1.apply(lambda x: len(str(x)))\n",
    "len_q2=test.question2.apply(lambda x: len(str(x)))\n",
    "x_test['diff_len'] = abs(len_q1-len_q2)\n",
    "\n",
    "len_char_q1=test.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "len_char_q2=test.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "x_test['diff_len_char']=abs(len_char_q1-len_char_q2)\n",
    "\n",
    "len_word_q1=test.question1.apply(lambda x: len(str(x).split()))\n",
    "len_word_q2=test.question2.apply(lambda x: len(str(x).split()))\n",
    "x_test['diff_len_word']=abs(len_word_q1-len_word_q2)\n",
    "#x_test['common_words'] = test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "\n",
    "x_test['fuzz_qratio'] = test.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_test['fuzz_WRatio'] = test.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_test['fuzz_partial_ratio'] = test.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_test['fuzz_partial_token_set_ratio'] = test.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_test['fuzz_partial_token_sort_ratio'] = test.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_test['fuzz_token_set_ratio'] = test.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "x_test['fuzz_token_sort_ratio'] = test.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "print 'test data is over'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin train_questions length:\n",
      "5500172\n",
      "after remove duplicates length:\n",
      "4789515\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "df1 = train[['question1']].copy()\n",
    "df2 = train[['question2']].copy()\n",
    "df1_test = test[['question1']].copy()\n",
    "df2_test = test[['question2']].copy()\n",
    "\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions = train_questions.append(df1_test)\n",
    "train_questions = train_questions.append(df2_test)\n",
    "\n",
    "print 'origin train_questions length:'\n",
    "\n",
    "print len(train_questions)\n",
    "\n",
    "#drop duplicated questions in train_questions\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "print 'after remove duplicates length:'\n",
    "\n",
    "print len(train_questions)\n",
    "\n",
    "#reset index of train_questions\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#construct new Series (index,question)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "\n",
    "train_cp = train.copy()\n",
    "test_cp = test.copy()\n",
    "\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "test_cp['is_duplicate'] = -1\n",
    "test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "\n",
    "x_train['q1_freq']=train_comb['q1_freq']\n",
    "x_train['q2_freq']=train_comb['q2_freq']\n",
    "#x_train['freq_diff']=abs(train_comb['q1_freq']-train_comb['q2_freq'])\n",
    "\n",
    "x_test['q1_freq']=test_comb['q1_freq']\n",
    "x_test['q2_freq']=test_comb['q2_freq']\n",
    "\n",
    "#x_test['freq_diff']=abs(test_comb['q1_freq']-test_comb['q2_freq'])\n",
    "\n",
    "print 'over'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebalance data\n",
      "1.2401128118\n",
      "0.191204686368\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "print 'rebalance data'\n",
    "\n",
    "y_train=train['is_duplicate'].values\n",
    "\n",
    "pos_train=x_train[y_train == 1]\n",
    "neg_train=x_train[y_train == 0]\n",
    "\n",
    "p = 0.165\n",
    "scale = (((len(pos_train) +0.0)/ (len(pos_train) + len(neg_train))) / p) - 1\n",
    "print scale\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print((len(pos_train)+0.0) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "x_train_r = pd.concat([pos_train, neg_train])\n",
    "y_train_r = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train\n",
    "\n",
    "x_train_r,x_valid_r,y_train_r,y_valid_r=train_test_split(\n",
    "    x_train_r,y_train_r,test_size=0.2, random_state=4242)\n",
    "\n",
    "print 'over'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost train\n",
      "[0]\ttrain-logloss:0.579371\tvalid-logloss:0.578989\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.292229\tvalid-logloss:0.290679\n",
      "[20]\ttrain-logloss:0.26221\tvalid-logloss:0.261798\n",
      "[30]\ttrain-logloss:0.253969\tvalid-logloss:0.254726\n",
      "[40]\ttrain-logloss:0.248542\tvalid-logloss:0.251109\n",
      "[50]\ttrain-logloss:0.244687\tvalid-logloss:0.248718\n",
      "[60]\ttrain-logloss:0.241939\tvalid-logloss:0.247099\n",
      "[70]\ttrain-logloss:0.239794\tvalid-logloss:0.246067\n",
      "[80]\ttrain-logloss:0.237486\tvalid-logloss:0.244945\n",
      "[90]\ttrain-logloss:0.235822\tvalid-logloss:0.244094\n",
      "[100]\ttrain-logloss:0.233922\tvalid-logloss:0.243143\n",
      "[110]\ttrain-logloss:0.232155\tvalid-logloss:0.241936\n",
      "[120]\ttrain-logloss:0.230098\tvalid-logloss:0.241002\n",
      "[130]\ttrain-logloss:0.228986\tvalid-logloss:0.240632\n",
      "[140]\ttrain-logloss:0.22728\tvalid-logloss:0.239746\n",
      "[150]\ttrain-logloss:0.225677\tvalid-logloss:0.239283\n",
      "[160]\ttrain-logloss:0.224348\tvalid-logloss:0.238609\n",
      "[170]\ttrain-logloss:0.222967\tvalid-logloss:0.238096\n",
      "[180]\ttrain-logloss:0.221138\tvalid-logloss:0.237623\n",
      "[190]\ttrain-logloss:0.219837\tvalid-logloss:0.23692\n",
      "[200]\ttrain-logloss:0.2184\tvalid-logloss:0.236455\n",
      "[210]\ttrain-logloss:0.217427\tvalid-logloss:0.236128\n",
      "[220]\ttrain-logloss:0.216317\tvalid-logloss:0.235774\n",
      "[230]\ttrain-logloss:0.214902\tvalid-logloss:0.235213\n",
      "[240]\ttrain-logloss:0.21363\tvalid-logloss:0.234516\n",
      "[250]\ttrain-logloss:0.21193\tvalid-logloss:0.233806\n",
      "[260]\ttrain-logloss:0.210434\tvalid-logloss:0.233114\n",
      "[270]\ttrain-logloss:0.209194\tvalid-logloss:0.232795\n",
      "[280]\ttrain-logloss:0.207789\tvalid-logloss:0.232011\n",
      "[290]\ttrain-logloss:0.206362\tvalid-logloss:0.231718\n",
      "[300]\ttrain-logloss:0.205021\tvalid-logloss:0.231295\n",
      "[310]\ttrain-logloss:0.204042\tvalid-logloss:0.230925\n",
      "[320]\ttrain-logloss:0.202882\tvalid-logloss:0.230345\n",
      "[330]\ttrain-logloss:0.201803\tvalid-logloss:0.229774\n",
      "[340]\ttrain-logloss:0.200605\tvalid-logloss:0.228897\n",
      "[350]\ttrain-logloss:0.199635\tvalid-logloss:0.228294\n",
      "[360]\ttrain-logloss:0.198767\tvalid-logloss:0.228004\n",
      "[370]\ttrain-logloss:0.197763\tvalid-logloss:0.227923\n",
      "[380]\ttrain-logloss:0.196396\tvalid-logloss:0.227524\n",
      "[390]\ttrain-logloss:0.195359\tvalid-logloss:0.22727\n",
      "[400]\ttrain-logloss:0.194247\tvalid-logloss:0.226877\n",
      "[410]\ttrain-logloss:0.193234\tvalid-logloss:0.226335\n",
      "[420]\ttrain-logloss:0.19226\tvalid-logloss:0.22614\n",
      "[430]\ttrain-logloss:0.190842\tvalid-logloss:0.225193\n",
      "[440]\ttrain-logloss:0.189639\tvalid-logloss:0.224751\n",
      "[450]\ttrain-logloss:0.188731\tvalid-logloss:0.224121\n",
      "[460]\ttrain-logloss:0.187689\tvalid-logloss:0.223717\n",
      "[470]\ttrain-logloss:0.186601\tvalid-logloss:0.223313\n",
      "[480]\ttrain-logloss:0.185558\tvalid-logloss:0.223153\n",
      "[490]\ttrain-logloss:0.184714\tvalid-logloss:0.222673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#xgboost 训练\n",
    "\n",
    "print 'Xgboost train'\n",
    "\n",
    "params={}\n",
    "params['objective']='binary:logistic'\n",
    "params['eval_metric']='logloss'\n",
    "params['eta']=0.02\n",
    "params['max_depth']=4\n",
    "\n",
    "d_train=xgb.DMatrix(x_train_r,label=y_train_r)\n",
    "d_valid=xgb.DMatrix(x_valid_r,label=y_valid_r)\n",
    "\n",
    "watchlist=[(d_train,'train'),(d_valid,'valid')]\n",
    "\n",
    "bst=xgb.train(params,d_train,500,watchlist,early_stopping_rounds=50,verbose_eval=10)\n",
    "\n",
    "d_test=xgb.DMatrix(x_test)\n",
    "p_test=bst.predict(d_test)\n",
    "\n",
    "#result\n",
    "sub=pd.DataFrame()\n",
    "sub['test_id']=test['test_id']\n",
    "sub['is_duplicate']=p_test\n",
    "sub.to_csv('xgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'taggers/averaged_perceptron_tagger/averaged_perceptro\n  n_tagger.pickle' not found.  Please use the NLTK Downloader to\n  obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/djh/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4850ed72f83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'What is like to have sex with cousin?.....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'taggers/averaged_perceptron_tagger/averaged_perceptro\n  n_tagger.pickle' not found.  Please use the NLTK Downloader to\n  obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/djh/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens=nltk.word_tokenize('What is like to have sex with cousin?.....')\n",
    "tokens=nltk.pos_tag(tokens)\n",
    "print tokens\n",
    "tree=nltk.ne_chunk(tokens)\n",
    "print tree\n",
    "\n",
    "print '---------------'\n",
    "tokens=nltk.word_tokenize('What is it like to have sex with your cousin?')\n",
    "tokens=nltk.pos_tag(tokens)\n",
    "print tokens\n",
    "tree=nltk.ne_chunk(tokens)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808575    How many keywords are there in PERL Programmin...\n",
      "808576           Is it true that there is life after death?\n",
      "808577                                    What's this coin?\n",
      "808578    I am having little hairfall problem but I want...\n",
      "808579        What is it like to have sex with your cousin?\n",
      "dtype: object\n",
      "-------- What is the step by step guide to invest in share market in india?\n",
      "by 0.251144897563\n",
      "cousin 0.0\n",
      "diamond 0.0\n",
      "guide 0.251144897563\n",
      "have 0.0\n",
      "in 0.502289795126\n",
      "india 0.251144897563\n",
      "invest 0.251144897563\n",
      "is 0.148330222231\n",
      "koh 0.0\n",
      "kohinoor 0.0\n",
      "like 0.0\n",
      "market 0.251144897563\n",
      "noor 0.0\n",
      "of 0.0\n",
      "sex 0.0\n",
      "share 0.251144897563\n",
      "step 0.502289795126\n",
      "story 0.0\n",
      "the 0.19100216797\n",
      "to 0.19100216797\n",
      "what 0.148330222231\n",
      "with 0.0\n",
      "-------- What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "by 0.0\n",
      "cousin 0.0\n",
      "diamond 0.370725138666\n",
      "guide 0.0\n",
      "have 0.0\n",
      "in 0.0\n",
      "india 0.0\n",
      "invest 0.0\n",
      "is 0.218956238963\n",
      "koh 0.370725138666\n",
      "kohinoor 0.370725138666\n",
      "like 0.0\n",
      "market 0.0\n",
      "noor 0.370725138666\n",
      "of 0.370725138666\n",
      "sex 0.0\n",
      "share 0.0\n",
      "step 0.0\n",
      "story 0.370725138666\n",
      "the 0.281946023564\n",
      "to 0.0\n",
      "what 0.218956238963\n",
      "with 0.0\n",
      "-------- What is like to have sex with cousin?\n",
      "by 0.0\n",
      "cousin 0.399168862237\n",
      "diamond 0.0\n",
      "guide 0.0\n",
      "have 0.399168862237\n",
      "in 0.0\n",
      "india 0.0\n",
      "invest 0.0\n",
      "is 0.235755560308\n",
      "koh 0.0\n",
      "kohinoor 0.0\n",
      "like 0.399168862237\n",
      "market 0.0\n",
      "noor 0.0\n",
      "of 0.0\n",
      "sex 0.399168862237\n",
      "share 0.0\n",
      "step 0.0\n",
      "story 0.0\n",
      "the 0.0\n",
      "to 0.30357820849\n",
      "what 0.235755560308\n",
      "with 0.399168862237\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus=[\"What is the step by step guide to invest in share market in india?\",\n",
    "         \"What is the story of Kohinoor (Koh-i-Noor) Diamond?\",\n",
    "         \"What is like to have sex with cousin?\"]\n",
    "vectorizer=CountVectorizer()\n",
    "transformer=TfidfTransformer()\n",
    "tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "word=vectorizer.get_feature_names()\n",
    "weight=tfidf.toarray()\n",
    "print train_qs.tail()\n",
    "for i in range(len(weight)):\n",
    "    print u\"--------\",corpus[i]\n",
    "    for j in range(len(word)):\n",
    "        print word[j],weight[i][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5892380908e-06\n"
     ]
    }
   ],
   "source": [
    "words=(\" \".join(train_qs)).lower().split()\n",
    "counts=Counter(words)\n",
    "weights={word:get_weight(count) for word,count in counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词：\n",
      "I\n",
      "How\n",
      "bad\n",
      "Youtube\n",
      "comments\n",
      "-------------\n",
      "wonderful\n",
      "read\n",
      "find\n",
      "I\n",
      "YouTube\n",
      "------------------\n",
      "0.0454071222115\n",
      "0.10363455497\n"
     ]
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "#s=SnowNLP(u'今天天气真好!')\n",
    "text = u'''\n",
    "How can I see all my bad Youtube comments\n",
    "'''\n",
    "\n",
    "text1 = u'''\n",
    "How do I read and find my wonderful YouTube comments\n",
    "'''\n",
    "\n",
    "s=SnowNLP(text)\n",
    "s1=SnowNLP(text1)\n",
    "\n",
    "print '关键词：'\n",
    "for keyword in s.keywords(5):\n",
    "    print keyword\n",
    "\n",
    "print '-------------'\n",
    "    \n",
    "for keyword in s1.keywords(5):\n",
    "    print keyword\n",
    "\n",
    "print '------------------'\n",
    "print s.sentiments\n",
    "print s1.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
