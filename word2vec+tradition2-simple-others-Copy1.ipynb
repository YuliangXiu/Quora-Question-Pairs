{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/scikit_learn-0.18.1-py2.7-linux-x86_64.egg/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,  Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from snownlp import SnowNLP\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import cPickle\n",
    "import gensim\n",
    "import math\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = 'data/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'cor_train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'cor_test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.02\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.loadtxt(\"data/data_1.csv\", delimiter=\",\")\n",
    "data_2 = np.loadtxt(\"data/data_2.csv\", delimiter=\",\")\n",
    "test_data_1 = np.loadtxt(\"data/test_data_1.csv\", delimiter=\",\")\n",
    "test_data_2 = np.loadtxt(\"data/test_data_2.csv\", delimiter=\",\")\n",
    "labels = np.loadtxt(\"data/labels.csv\", delimiter=\",\")\n",
    "embedding_matrix = np.load('data/embedding_matrix.npy')\n",
    "nb_words = 120501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle('data/x_train_norm.pkl')\n",
    "x_test = pd.read_pickle('data/x_test_norm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_other = pd.read_csv('data/train_features.csv')\n",
    "x_train = pd.concat([x_train_other.ix[:,2:],x_train], axis=1)\n",
    "x_test_other = pd.read_csv('data/test_features.csv')\n",
    "x_test = pd.concat([x_test_other.ix[:,2:],x_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_columns = x_train.columns\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "x_train['norm_wmd'] = x_train['norm_wmd'].replace(np.inf, 1.48)\n",
    "x_test['norm_wmd'] = x_test['norm_wmd'].replace(np.inf, 1.48)\n",
    "x_train['wmd'] = x_train['wmd'].replace(np.inf, 6.2)\n",
    "x_test['wmd'] = x_test['wmd'].replace(np.inf, 6.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((x_train, x_test)))\n",
    "x_train = ss.transform(x_train)\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "x_train = pd.DataFrame(data=x_train, columns=data_columns)\n",
    "x_test = pd.DataFrame(data=x_test, columns=data_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "\n",
    "data_3_train = np.vstack((x_train.values[idx_train],x_train.values[idx_train]))\n",
    "data_3_val = np.vstack((x_train.values[idx_val],x_train.values[idx_val]))\n",
    "\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_lstm = 250\n",
    "num_dense = 300\n",
    "rate_drop_lstm = 0.4\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer =  Bidirectional(LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "z1 = Input(shape=(x_train.shape[1],), dtype='float32')\n",
    "z1_dense = Dense(num_dense/2, activation=act)(z1)\n",
    "\n",
    "merged = concatenate([x1, y1, z1_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_250_300_0.40_0.30\n",
      "Train on 792408 samples, validate on 16172 samples\n",
      "Epoch 1/200\n",
      "792408/792408 [==============================] - 275s - loss: 0.2405 - acc: 0.8288 - val_loss: 0.2083 - val_acc: 0.8585\n",
      "Epoch 2/200\n",
      "792408/792408 [==============================] - 274s - loss: 0.1953 - acc: 0.8554 - val_loss: 0.1988 - val_acc: 0.8353\n",
      "Epoch 3/200\n",
      "792408/792408 [==============================] - 269s - loss: 0.1860 - acc: 0.8627 - val_loss: 0.1862 - val_acc: 0.8742\n",
      "Epoch 4/200\n",
      "792408/792408 [==============================] - 259s - loss: 0.1788 - acc: 0.8682 - val_loss: 0.1883 - val_acc: 0.8843\n",
      "Epoch 5/200\n",
      "792408/792408 [==============================] - 260s - loss: 0.1726 - acc: 0.8733 - val_loss: 0.1780 - val_acc: 0.8635\n",
      "Epoch 6/200\n",
      "792408/792408 [==============================] - 260s - loss: 0.1678 - acc: 0.8770 - val_loss: 0.1748 - val_acc: 0.8727\n",
      "Epoch 7/200\n",
      "792408/792408 [==============================] - 260s - loss: 0.1631 - acc: 0.8809 - val_loss: 0.1747 - val_acc: 0.8850\n",
      "Epoch 8/200\n",
      "792408/792408 [==============================] - 259s - loss: 0.1592 - acc: 0.8837 - val_loss: 0.1722 - val_acc: 0.8843\n",
      "Epoch 9/200\n",
      "792408/792408 [==============================] - 259s - loss: 0.1551 - acc: 0.8871 - val_loss: 0.1723 - val_acc: 0.8842\n",
      "Epoch 10/200\n",
      "792408/792408 [==============================] - 260s - loss: 0.1517 - acc: 0.8898 - val_loss: 0.1704 - val_acc: 0.8823\n",
      "Epoch 11/200\n",
      "792408/792408 [==============================] - 259s - loss: 0.1485 - acc: 0.8918 - val_loss: 0.1729 - val_acc: 0.8910\n",
      "Epoch 12/200\n",
      "792408/792408 [==============================] - 257s - loss: 0.1454 - acc: 0.8947 - val_loss: 0.1746 - val_acc: 0.8917\n",
      "Epoch 13/200\n",
      "548864/792408 [===================>..........] - ETA: 78s - loss: 0.1424 - acc: 0.8968"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, z1], \\\n",
    "        outputs=preds)\n",
    "# model.load_weights(bst_model_path)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_loss',save_best_only=True, save_weights_only=True)\n",
    "# model.load_weights(bst_model_path)\n",
    "hist = model.fit([data_1_train, data_2_train, data_3_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, data_3_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = pd.DataFrame({'epoch': [ i + 1 for i in hist.epoch ],\n",
    "                    'training': hist.history['acc'],\n",
    "                    'validation': hist.history['val_acc'],\n",
    "                    'loss': hist.history['loss'],\n",
    "                   'val_loss': hist.history['val_loss']})\n",
    "ax = acc.ix[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim([0.0,1.0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "model.load_weights(bst_model_path)\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, x_test.values], batch_size=5000, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, x_test.values], batch_size=5000, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "test_ids = np.arange(len(preds))\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print '%.4f_'%(bst_val_score)+STAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
