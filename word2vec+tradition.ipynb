{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/scikit_learn-0.18.1-py2.7-linux-x86_64.egg/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,  Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from snownlp import SnowNLP\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import cPickle\n",
    "import gensim\n",
    "import math\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = 'data/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'cor_train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'cor_test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.02\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始计算 glove 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404290 texts in train.csv\n",
      "Found 2345796 texts in test.csv\n",
      "Found 120500 unique tokens\n",
      "('Shape of data tensor:', (404290, 30))\n",
      "('Shape of label tensor:', (404290,))\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## text to sequence numbers\n",
    "########################################\n",
    "train_1 = [] \n",
    "train_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        train_1.append(text_to_wordlist(values[3]))\n",
    "        train_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(train_1))\n",
    "\n",
    "test_1 = []\n",
    "test_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_1.append(text_to_wordlist(values[1]))\n",
    "        test_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_1 + train_2 + test_1 + test_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(train_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(train_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 61789\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始计算传统特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## traditional features processing\n",
    "########################################\n",
    "\n",
    "train=pd.read_csv(TRAIN_DATA_FILE)\n",
    "test=pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "train_qs=pd.Series(train['question1'].tolist()+\n",
    "                   train['question2'].tolist()).astype(str)\n",
    "\n",
    "test_qs=pd.Series(test['question1'].tolist()+\n",
    "                  test['question2'].tolist()).astype(str)\n",
    "\n",
    "stops=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## traditional features function\n",
    "########################################\n",
    "\n",
    "#计算两句话的共有词\n",
    "def word_match_share(row):\n",
    "    q1words={}\n",
    "    q2words={}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:#如果不是stopwords则存入q1words=>(key,value)\n",
    "            q1words[word]=1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word]=1\n",
    "    if len(q1words)==0 or len(q2words)==0:\n",
    "        return 0\n",
    "    shared_words_q1=[w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_q2=[w for w in q2words.keys() if w in q1words]\n",
    "    R=(len(shared_words_q1)+len(shared_words_q2)+0.0)/(len(q1words)+len(q2words))\n",
    "    return R\n",
    "\n",
    "def get_weight(count,eps=10000,min_count=2):\n",
    "    if count<min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1.0/(count+eps)\n",
    "    \n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + \\\n",
    "                    [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "        \n",
    "    R = (np.sum(shared_weights)+0.0) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def sentence_sentiment_diff(row):\n",
    "    s1=SnowNLP(str(row['question1'])).sentiments\n",
    "    s2=SnowNLP(str(row['question2'])).sentiments\n",
    "    return (s1-s2)*(s1-s2)\n",
    "\n",
    "words=(\" \".join(train_qs)).lower().split()\n",
    "counts=Counter(words)\n",
    "weights={word:get_weight(count) for word,count in counts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如果已经保存特征，那么下面就不用执行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame()\n",
    "x_test = pd.DataFrame()\n",
    "\n",
    "x_train['word_match'] = train.apply(word_match_share,axis=1,raw=True)\n",
    "x_test['word_match'] = test.apply(word_match_share, axis=1, raw=True)\n",
    "x_train['tfidf_word_match'] = train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "x_test['tfidf_word_match'] = test.apply(tfidf_word_match_share,axis=1,raw=True)\n",
    "x_train['sentiment']=train.apply(sentence_sentiment_diff,axis=1,raw=True)\n",
    "x_test['sentiment']=test.apply(sentence_sentiment_diff,axis=1,raw=True)\n",
    "\n",
    "len_q1=train.question1.apply(lambda x: len(str(x)))\n",
    "len_q2=train.question2.apply(lambda x: len(str(x)))\n",
    "x_train['diff_len'] = abs(len_q1-len_q2)\n",
    "\n",
    "len_char_q1=train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "len_char_q2=train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "x_train['diff_len_char']=abs(len_char_q1-len_char_q2)\n",
    "\n",
    "len_word_q1=train.question1.apply(lambda x: len(str(x).split()))\n",
    "len_word_q2=train.question2.apply(lambda x: len(str(x).split()))\n",
    "x_train['diff_len_word']=abs(len_word_q1-len_word_q2)\n",
    "\n",
    "len_q1=test.question1.apply(lambda x: len(str(x)))\n",
    "len_q2=test.question2.apply(lambda x: len(str(x)))\n",
    "x_test['diff_len'] = abs(len_q1-len_q2)\n",
    "\n",
    "len_char_q1=test.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "len_char_q2=test.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "x_test['diff_len_char']=abs(len_char_q1-len_char_q2)\n",
    "\n",
    "len_word_q1=test.question1.apply(lambda x: len(str(x).split()))\n",
    "len_word_q2=test.question2.apply(lambda x: len(str(x).split()))\n",
    "x_test['diff_len_word']=abs(len_word_q1-len_word_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin train_questions length:\n",
      "5500172\n",
      "after remove duplicates length:\n",
      "4789032\n"
     ]
    }
   ],
   "source": [
    "df1 = train[['question1']].copy()\n",
    "df2 = train[['question2']].copy()\n",
    "df1_test = test[['question1']].copy()\n",
    "df2_test = test[['question2']].copy()\n",
    "\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions = train_questions.append(df1_test)\n",
    "train_questions = train_questions.append(df2_test)\n",
    "\n",
    "print 'origin train_questions length:'\n",
    "\n",
    "print len(train_questions)\n",
    "\n",
    "#drop duplicated questions in train_questions\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "print 'after remove duplicates length:'\n",
    "\n",
    "print len(train_questions)\n",
    "\n",
    "#reset index of train_questions\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#construct new Series (index,question)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "\n",
    "train_cp = train.copy()\n",
    "test_cp = test.copy()\n",
    "\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "test_cp['is_duplicate'] = -1\n",
    "test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "\n",
    "comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "\n",
    "x_train['q1_freq']=train_comb['q1_freq']\n",
    "x_train['q2_freq']=train_comb['q2_freq']\n",
    "\n",
    "x_test['q1_freq']=test_comb['q1_freq']\n",
    "x_test['q2_freq']=test_comb['q2_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.to_pickle('data/x_train.pkl')\n",
    "x_test.to_pickle('data/x_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接调用保存好的 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_hand_features = pd.read_pickle('data/x_train.pkl')\n",
    "test_hand_features = pd.read_pickle('data/x_test.pkl')\n",
    "x_train = train_hand_features[['word_match','tfidf_word_match','sentiment','diff_len','diff_len_char','diff_len_word','q1_freq','q2_freq']]\n",
    "x_test = test_hand_features[['word_match','tfidf_word_match','sentiment','diff_len','diff_len_char','diff_len_word','q1_freq','q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加新的 feature\n",
    "interdict_train=pd.read_csv('interdict_train.csv',index_col=0)\n",
    "interdict_test=pd.read_csv('interdict_test.csv',index_col=0)\n",
    "x_train = pd.concat([x_train, interdict_train], axis=1)\n",
    "x_test = pd.concat([x_test, interdict_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_columns = x_train.columns\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((x_train, x_test)))\n",
    "x_train = ss.transform(x_train)\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "x_train = pd.DataFrame(data=x_train, columns=data_columns)\n",
    "x_test = pd.DataFrame(data=x_test, columns=data_columns)\n",
    "\n",
    "x_train.to_pickle('data/x_train_norm.pkl')\n",
    "x_test.to_pickle('data/x_test_norm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle('data/x_train_norm.pkl')\n",
    "x_test = pd.read_pickle('data/x_test_norm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "\n",
    "data_3_train = np.vstack((x_train.values[idx_train],x_train.values[idx_train]))\n",
    "data_3_val = np.vstack((x_train.values[idx_val],x_train.values[idx_val]))\n",
    "\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer =  Bidirectional(LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "z1 = Input(shape=(10,), dtype='float32')\n",
    "z1_dense = Dense(num_dense/2, activation=act)(z1)\n",
    "\n",
    "merged = concatenate([x1, y1, z1_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_222_139_0.37_0.16\n",
      "Train on 792408 samples, validate on 16172 samples\n",
      "Epoch 1/200\n",
      "792408/792408 [==============================] - 203s - loss: 0.2377 - acc: 0.8312 - val_loss: 0.2224 - val_acc: 0.8141\n",
      "Epoch 2/200\n",
      "792408/792408 [==============================] - 202s - loss: 0.2014 - acc: 0.8513 - val_loss: 0.1983 - val_acc: 0.8577\n",
      "Epoch 3/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1926 - acc: 0.8577 - val_loss: 0.1972 - val_acc: 0.8709\n",
      "Epoch 4/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1849 - acc: 0.8634 - val_loss: 0.1875 - val_acc: 0.8709\n",
      "Epoch 5/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1785 - acc: 0.8678 - val_loss: 0.1887 - val_acc: 0.8774\n",
      "Epoch 6/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1731 - acc: 0.8722 - val_loss: 0.1818 - val_acc: 0.8745\n",
      "Epoch 7/200\n",
      "792408/792408 [==============================] - 200s - loss: 0.1684 - acc: 0.8760 - val_loss: 0.1825 - val_acc: 0.8799\n",
      "Epoch 8/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1635 - acc: 0.8798 - val_loss: 0.1795 - val_acc: 0.8776\n",
      "Epoch 9/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1593 - acc: 0.8829 - val_loss: 0.1762 - val_acc: 0.8787\n",
      "Epoch 10/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1558 - acc: 0.8861 - val_loss: 0.1780 - val_acc: 0.8836\n",
      "Epoch 11/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1522 - acc: 0.8887 - val_loss: 0.1787 - val_acc: 0.8831\n",
      "Epoch 12/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1490 - acc: 0.8914 - val_loss: 0.1851 - val_acc: 0.8844\n",
      "Epoch 13/200\n",
      "792408/792408 [==============================] - 201s - loss: 0.1465 - acc: 0.8939 - val_loss: 0.1790 - val_acc: 0.8834\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, z1], \\\n",
    "        outputs=preds)\n",
    "# model.load_weights(bst_model_path)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_loss',save_best_only=True, save_weights_only=True)\n",
    "# model.load_weights(bst_model_path)\n",
    "hist = model.fit([data_1_train, data_2_train, data_3_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, data_3_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345796/2345796 [==============================] - 212s   \n",
      "2345796/2345796 [==============================] - 212s   \n"
     ]
    }
   ],
   "source": [
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "model.load_weights(bst_model_path)\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, x_test.values], batch_size=5000, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, x_test.values], batch_size=5000, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
