{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def word_match_share(row, stops=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(row):\n",
    "    wic = set(row['question1']).intersection(set(row['question2']))\n",
    "    uw = set(row['question1']).union(row['question2'])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(row):\n",
    "    return len(set(row['question1']).intersection(set(row['question2'])))\n",
    "\n",
    "def total_unique_words(row):\n",
    "    return len(set(row['question1']).union(row['question2']))\n",
    "\n",
    "def total_unq_words_stop(row, stops):\n",
    "    return len([x for x in set(row['question1']).union(row['question2']) if x not in stops])\n",
    "\n",
    "def wc_diff(row):\n",
    "    return abs(len(row['question1']) - len(row['question2']))\n",
    "\n",
    "def wc_ratio(row):\n",
    "    l1 = len(row['question1'])*1.0 \n",
    "    l2 = len(row['question2'])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(row):\n",
    "    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n",
    "\n",
    "def wc_ratio_unique(row):\n",
    "    l1 = len(set(row['question1'])) * 1.0\n",
    "    l2 = len(set(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique_stop(row, stops=None):\n",
    "    return abs(len([x for x in set(row['question1']) if x not in stops]) - len([x for x in set(row['question2']) if x not in stops]))\n",
    "\n",
    "def wc_ratio_unique_stop(row, stops=None):\n",
    "    l1 = len([x for x in set(row['question1']) if x not in stops])*1.0 \n",
    "    l2 = len([x for x in set(row['question2']) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(row):\n",
    "    if not row['question1'] or not row['question2']:\n",
    "        return np.nan\n",
    "    return int(row['question1'][0] == row['question2'][0])\n",
    "\n",
    "def char_diff(row):\n",
    "    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n",
    "\n",
    "def char_ratio(row):\n",
    "    l1 = len(''.join(row['question1'])) \n",
    "    l2 = len(''.join(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def char_diff_unique_stop(row, stops=None):\n",
    "    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(''.join([x for x in set(row['question2']) if x not in stops])))\n",
    "\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def tfidf_word_match_share_stops(row, stops=None, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "\n",
    "def build_features(data, stops, weights):\n",
    "    X = pd.DataFrame()\n",
    "    f = functools.partial(word_match_share, stops=stops)\n",
    "    X['word_match'] = data.apply(f, axis=1, raw=True) #1\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "    X['tfidf_wm'] = data.apply(f, axis=1, raw=True) #2\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share_stops, stops=stops, weights=weights)\n",
    "    X['tfidf_wm_stops'] = data.apply(f, axis=1, raw=True) #3\n",
    "\n",
    "    X['jaccard'] = data.apply(jaccard, axis=1, raw=True) #4\n",
    "    X['wc_diff'] = data.apply(wc_diff, axis=1, raw=True) #5\n",
    "    X['wc_ratio'] = data.apply(wc_ratio, axis=1, raw=True) #6\n",
    "    X['wc_diff_unique'] = data.apply(wc_diff_unique, axis=1, raw=True) #7\n",
    "    X['wc_ratio_unique'] = data.apply(wc_ratio_unique, axis=1, raw=True) #8\n",
    "\n",
    "    f = functools.partial(wc_diff_unique_stop, stops=stops)    \n",
    "    X['wc_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #9\n",
    "    f = functools.partial(wc_ratio_unique_stop, stops=stops)    \n",
    "    X['wc_ratio_unique_stop'] = data.apply(f, axis=1, raw=True) #10\n",
    "\n",
    "    X['same_start'] = data.apply(same_start_word, axis=1, raw=True) #11\n",
    "    X['char_diff'] = data.apply(char_diff, axis=1, raw=True) #12\n",
    "\n",
    "    f = functools.partial(char_diff_unique_stop, stops=stops) \n",
    "    X['char_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #13\n",
    "\n",
    "#     X['common_words'] = data.apply(common_words, axis=1, raw=True)  #14\n",
    "    X['total_unique_words'] = data.apply(total_unique_words, axis=1, raw=True)  #15\n",
    "\n",
    "    f = functools.partial(total_unq_words_stop, stops=stops)\n",
    "    X['total_unq_words_stop'] = data.apply(f, axis=1, raw=True)  #16\n",
    "    \n",
    "    X['char_ratio'] = data.apply(char_ratio, axis=1, raw=True) #17    \n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='XGB with Handcrafted Features')\n",
    "    parser.add_argument('--save', type=str, default='XGB_leaky',\n",
    "                        help='save_file_names')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df_train = pd.read_csv('../data/train_features.csv', encoding=\"ISO-8859-1\")\n",
    "    X_train_ab = df_train.iloc[:, 2:-1]\n",
    "    X_train_ab = X_train_ab.drop('euclidean_distance', axis=1)\n",
    "    X_train_ab = X_train_ab.drop('jaccard_distance', axis=1)\n",
    "\n",
    "    df_train = pd.read_csv('../data/train.csv')\n",
    "    df_train = df_train.fillna(' ')\n",
    "\n",
    "    df_test = pd.read_csv('../data/test.csv')\n",
    "    ques = pd.concat([df_train[['question1', 'question2']], \\\n",
    "        df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(ques.shape[0]):\n",
    "            q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "            q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "    def q1_freq(row):\n",
    "        return(len(q_dict[row['question1']]))\n",
    "        \n",
    "    def q2_freq(row):\n",
    "        return(len(q_dict[row['question2']]))\n",
    "        \n",
    "    def q1_q2_intersect(row):\n",
    "        return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "    df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    df_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n",
    "    df_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "    df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    df_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n",
    "    df_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "    test_leaky = df_test.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "    del df_test\n",
    "\n",
    "    train_leaky = df_train.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "\n",
    "    # explore\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "    df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "    train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist())\n",
    "\n",
    "    words = [x for y in train_qs for x in y]\n",
    "    counts = Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "    print('Building Features')\n",
    "    X_train = build_features(df_train, stops, weights)\n",
    "    X_train = pd.concat((X_train, X_train_ab, train_leaky), axis=1)\n",
    "    y_train = df_train['is_duplicate'].values\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=4242)\n",
    "\n",
    "    #UPDownSampling\n",
    "    pos_train = X_train[y_train == 1]\n",
    "    neg_train = X_train[y_train == 0]\n",
    "    X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8*len(pos_train))], neg_train))\n",
    "    y_train = np.array([0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8*len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "    print(np.mean(y_train))\n",
    "    del pos_train, neg_train\n",
    "\n",
    "    pos_valid = X_valid[y_valid == 1]\n",
    "    neg_valid = X_valid[y_valid == 0]\n",
    "    X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "    y_valid = np.array([0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "    print(np.mean(y_valid))\n",
    "    del pos_valid, neg_valid\n",
    "\n",
    "\n",
    "    params = {}\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['eta'] = 0.02\n",
    "    params['max_depth'] = 7\n",
    "    params['subsample'] = 0.6\n",
    "    params['base_score'] = 0.2\n",
    "    # params['scale_pos_weight'] = 0.2\n",
    "\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "    print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "    bst.save_model(args.save + '.mdl')\n",
    "\n",
    "\n",
    "    print('Building Test Features')\n",
    "    df_test = pd.read_csv('../data/test_features.csv', encoding=\"ISO-8859-1\")\n",
    "    x_test_ab = df_test.iloc[:, 2:-1]\n",
    "    x_test_ab = x_test_ab.drop('euclidean_distance', axis=1)\n",
    "    x_test_ab = x_test_ab.drop('jaccard_distance', axis=1)\n",
    "    \n",
    "    df_test = pd.read_csv('../data/test.csv')\n",
    "    df_test = df_test.fillna(' ')\n",
    "\n",
    "    df_test['question1'] = df_test['question1'].map(lambda x: str(x).lower().split())\n",
    "    df_test['question2'] = df_test['question2'].map(lambda x: str(x).lower().split())\n",
    "    \n",
    "    x_test = build_features(df_test, stops, weights)\n",
    "    x_test = pd.concat((x_test, x_test_ab, test_leaky), axis=1)\n",
    "    d_test = xgb.DMatrix(x_test)\n",
    "    p_test = bst.predict(d_test)\n",
    "    sub = pd.DataFrame()\n",
    "    sub['test_id'] = df_test['test_id']\n",
    "    sub['is_duplicate'] = p_test\n",
    "    sub.to_csv('../predictions/' + args.save + '.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a_name = 'combine'\n",
    "b_name = '0.1699_lstm_225_140_0.37_0.16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(a_name + '.csv')\n",
    "b = pd.read_csv(b_name + '.csv')\n",
    "\n",
    "ids = a['test_id'].values\n",
    "cfd = (a['is_duplicate'].values + b['is_duplicate'].values)/2.0\n",
    "\n",
    "comb = pd.DataFrame({'test_id':ids, 'is_duplicate':cfd})\n",
    "\n",
    "comb.to_csv('combine.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'question1', u'question2', u'len_q1', u'len_q2', u'diff_len',\n",
      "       u'len_char_q1', u'len_char_q2', u'len_word_q1', u'len_word_q2',\n",
      "       u'common_words', u'fuzz_qratio', u'fuzz_WRatio', u'fuzz_partial_ratio',\n",
      "       u'fuzz_partial_token_set_ratio', u'fuzz_partial_token_sort_ratio',\n",
      "       u'fuzz_token_set_ratio', u'fuzz_token_sort_ratio', u'wmd', u'norm_wmd',\n",
      "       u'cosine_distance', u'cityblock_distance', u'jaccard_distance',\n",
      "       u'canberra_distance', u'euclidean_distance', u'minkowski_distance',\n",
      "       u'braycurtis_distance', u'skew_q1vec', u'skew_q2vec', u'kur_q1vec',\n",
      "       u'kur_q2vec'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "trainset = pd.read_csv('data/train_features.csv')\n",
    "print trainset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.49995284328\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print skew([1,2,1000,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
